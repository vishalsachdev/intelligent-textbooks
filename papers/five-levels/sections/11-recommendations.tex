\section{Recommendations}

Educational institutions considering intelligent textbook adoption should begin with small, focused pilot projects before large-scale deployment. This approach enables learning from controlled experiments while managing risk and building institutional capacity. We recommend the following pilot project categories:

\subsection{Learning Graph-Centered Textbook Creation}

Institutions should pilot the creation of intelligent textbooks using AI-assisted workflows centered around learning graphs~\cite{mccreary2025claudeskills}. Modern AI coding tools now include specialized skills for generating educational content, including:

\begin{itemize}
    \item Creation of ultra-high quality course descriptions with learning objectives classified using the 2001 Bloom Taxonomy
    \item Automated extraction and inference of concept lists from course descriptions
    \item Generation of concept dependency relationships for each concept and clear identification of foundation concepts (prerequisites)
    \item Creation of learning graph visualizations and editing of learning graphs
    \item Creation of chapter structure for balance (relative chapter size) and respect for concept dependency
    \item AI-assisted chapter content generation aligned with concept sequences and suggested MicroSims
    \item Generation of MicroSims with each chapter using both search, customization and  creation tools
    \item Automated generation of supplementary material such as glossary for terms, FAQs, quizzes and slides
\end{itemize}

A successful pilot involves selecting a single course, building its learning graph, and iteratively developing content with AI assistance. Large libraries of detailed rules to implement each step are already available for testing~\cite{mccreary2025claudeskills}.  This provides hands-on experience with Level 2 capabilities while producing reusable course materials. Importantly, this pilot requires no student data collection, enabling institutions to build expertise before crossing the privacy threshold.

\subsection{A/B Testing of Interactive Content}

Rigorous evaluation requires controlled comparison. Institutions should conduct side-by-side evaluations of students learning with and without interactive textbooks featuring extensive MicroSims (interactive microsimulations). Key elements include:

\begin{itemize}
    \item Random assignment of students to treatment and control groups
    \item Matched courses with equivalent learning objectives
    \item Pre- and post-assessments measuring learning outcomes
    \item Student engagement metrics (time on task, completion rates)
    \item Qualitative feedback on learning experience
\end{itemize}

Such studies generate evidence specific to the institution's context and student population. Results inform whether the benefits of interactive content justify development investment and guide priorities for future MicroSim development.

\subsection{Learning Record Store Vendor Evaluation}

Before implementing Level 3+ systems, institutions must establish secure infrastructure for student data. A pilot project should systematically evaluate Learning Record Store (LRS) vendors against institutional requirements:

\begin{itemize}
    \item FERPA compliance and data residency options
    \item GDPR compliance for international students
    \item xAPI conformance testing certification
    \item Data export capabilities for student portability rights
    \item Integration with existing institutional systems (LMS, SIS)
    \item Security audit history and incident response procedures
    \item Data retention and deletion policies
\end{itemize}

This evaluation establishes the privacy infrastructure necessary for responsible adoption of adaptive systems. Even if Level 3 adoption is years away, understanding vendor options informs strategic planning.

\subsection{AI-Powered Learning Analytics}

Once LRS infrastructure exists, institutions can pilot AI analysis of learning data to improve course design. Working with appropriately anonymized or consented data, pilot projects can explore:

\begin{itemize}
    \item Identification of content sequences where students consistently struggle
    \item Detection of concepts requiring additional prerequisite coverage
    \item Analysis of learning path patterns that predict success
    \item Recommendations for MicroSim placement based on engagement data
    \item Early warning indicators for at-risk students
\end{itemize}

This pilot builds institutional capacity for data-informed course improvement while developing governance frameworks for AI-assisted decision-making in education.

\subsection{Staff AI Literacy Programs}

Perhaps most fundamentally, institutions should invest in comprehensive AI literacy education for all staff---not only technology specialists but faculty, administrators, and support personnel. Effective programs address:

\begin{itemize}
    \item Foundational understanding of how large language models work
    \item Practical applications of AI in content creation and assessment
    \item Critical evaluation of AI-generated content for accuracy and bias
    \item Ethical considerations in AI-assisted education
    \item Privacy implications at each level of intelligent textbook adoption
    \item Hands-on workshops with AI tools relevant to each role
\end{itemize}

Staff literacy enables informed participation in adoption decisions and builds the human capacity essential for responsible AI integration. Institutions where only a small technical team understands AI capabilities risk either over-enthusiastic adoption without adequate oversight or fear-based resistance that forecloses beneficial applications.

\subsection{Pilot Project Governance}

Regardless of which pilots an institution pursues, successful execution requires appropriate governance:

\begin{itemize}
    \item Clear success criteria defined before pilot launch
    \item Designated pilot owners with authority and accountability
    \item Regular progress reviews with institutional leadership
    \item Documentation of lessons learned for future initiatives
    \item Planned decision points for scaling, pivoting, or discontinuing
\end{itemize}

Small pilots with clear governance generate the institutional knowledge necessary for confident larger-scale adoption decisions.
